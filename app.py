# -*- coding: utf-8 -*-
"""
è®ºæ–‡å†™ä½œåŠ©æ‰‹ - Webç•Œé¢ï¼ˆè‡ªåŠ¨æ•´åˆç‰ˆï¼‰
Paper Writing Assistant - Web UI (Auto-Integration)
"""

import streamlit as st
import os
import sys
import tempfile
import requests
import json
import pandas as pd
import sqlite3
import io
from pathlib import Path
import shutil
import time
import traceback

# æ·»åŠ srcåˆ°è·¯å¾„
project_root = str(Path(__file__).parent.resolve())
src_path = os.path.join(project_root, "src")
sys.path.insert(0, src_path)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="è®ºæ–‡å†™ä½œåŠ©æ‰‹",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded",
)

# å¯¼å…¥æ¨¡å—
from analyzer import analyze_journal_style
from analyzer.ai_deepseek_analyzer import analyze_journal_style_with_ai
from literature import create_literature_database, LiteratureDatabaseManager
# from coordinator import MultiAgentCoordinator  # æš‚æ—¶æ³¨é‡Šæ‰
# from integrator import DraftIntegrator  # æš‚æ—¶æ³¨é‡Šæ‰

# æ ‡é¢˜
st.title("ğŸ“ è®ºæ–‡å†™ä½œåŠ©æ‰‹")
st.markdown("åŸºäºAIçš„å¤šä»£ç†å­¦æœ¯è®ºæ–‡å†™ä½œç³»ç»Ÿ")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")

    # Clean the URL - remove /v1 suffix if user entered it
    default_url = ""
    api_url = st.text_input(
        "APIä»£ç†åœ°å€",
        value=default_url,
        help="å®Œæ•´çš„APIä»£ç†åœ°å€ï¼ŒåŒ…å«/v1è·¯å¾„",
        key="api_url_input",
    )

    # åªå»é™¤æœ«å°¾çš„æ–œæ ï¼Œä¿ç•™/v1
    if api_url.endswith("/"):
        api_url = api_url[:-1]

    api_key = st.text_input(
        "API Key",
        type="password",
        value="",
        help="è¾“å…¥ä½ çš„API Keyç”¨äºè®¿é—®AIæ¨¡å‹",
        key="api_key_input",
    )

    if api_key and api_url:
        st.success("APIå·²é…ç½®")
    elif api_url and not api_key:
        st.warning("è¯·è¾“å…¥API Key")
    elif api_key and not api_url:
        st.warning("è¯·è¾“å…¥APIä»£ç†åœ°å€")
    else:
        st.info("è¯·é…ç½®APIè¿æ¥")

    # Verify API connection
    if st.button("éªŒè¯APIè¿æ¥", type="primary"):
        if not api_url or not api_key:
            st.error("è¯·åŒæ—¶è¾“å…¥APIåœ°å€å’ŒAPI Key")
        else:
            try:
                # Test connection - remove trailing slash if present
                base_url = api_url.rstrip("/")
                test_url = f"{base_url}/models"

                headers = {"Authorization": f"Bearer {api_key}"}
                response = requests.get(test_url, headers=headers, timeout=5)

                if response.status_code == 200:
                    try:
                        data = response.json()
                        models = data.get("data", [])
                        st.success("APIè¿æ¥æˆåŠŸï¼")
                        st.write(f"å¯ç”¨æ¨¡å‹: {len(models)} ä¸ª")
                        if models:
                            with st.expander("æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨"):
                                for m in models[:10]:
                                    st.write(f"- {m.get('id', 'Unknown')}")
                                if len(models) > 10:
                                    st.write(f"... å…± {len(models)} ä¸ªæ¨¡å‹")
                    except Exception:
                        # è¿”å›200ä½†ä¸æ˜¯JSONï¼Œå¯èƒ½æ˜¯ä»£ç†è¿”å›äº†å…¶ä»–å†…å®¹
                        st.warning(
                            "APIè¿æ¥æ­£å¸¸ï¼Œä½†è¿”å›æ ¼å¼å¼‚å¸¸ã€‚ä»£ç†å¯èƒ½ä¸æ”¯æŒ/modelsç«¯ç‚¹ã€‚"
                        )
                elif response.status_code == 401:
                    st.error("è¿æ¥å¤±è´¥: API Keyæ— æ•ˆ")
                elif response.status_code == 404:
                    st.error("è¿æ¥å¤±è´¥: ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥APIåœ°å€æ˜¯å¦æ­£ç¡®")
                else:
                    st.error(f"è¿æ¥å¤±è´¥: HTTP {response.status_code}")
            except requests.exceptions.ConnectionError:
                st.error("è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
            except Exception as e:
                st.error(f"è¿æ¥å¤±è´¥: {str(e)[:100]}")

    # DeepSeeké…ç½® - ç”¨äºå¢å¼ºç‰¹å¾æå–
    st.header("ğŸ¤– DeepSeek é…ç½®")
    st.markdown("ç”¨äºé«˜çº§é£æ ¼ç‰¹å¾æå–å’ŒRAGå¢å¼ºåˆ†æ")

    deepseek_api_key = st.text_input(
        "DeepSeek API Key",
        type="password",
        value="",
        help="è¾“å…¥DeepSeek API Keyç”¨äºå¢å¼ºç‰¹å¾æå–åŠŸèƒ½",
        key="deepseek_api_key_input",
    )

    deepseek_base_url = st.text_input(
        "DeepSeek Base URL",
        value="https://api.deepseek.com/v1",
        help="DeepSeek APIçš„åŸºç¡€URL",
        key="deepseek_base_url_input",
    )

    if deepseek_api_key:
        st.success("âœ… DeepSeek APIå·²é…ç½®")
    else:
        st.info("ğŸ’¡ é…ç½®DeepSeek APIä»¥å¯ç”¨å¢å¼ºç‰¹å¾æå–")

    st.info("""
    ğŸ“Œ ä½¿ç”¨è¯´æ˜

    1. ç¡®ä¿APIä»£ç†æ­£åœ¨è¿è¡Œ
    2. å¡«å…¥æ­£ç¡®çš„API Key
    3. é…ç½®DeepSeek APIç”¨äºå¢å¼ºç‰¹å¾æå–
    4. ä¸Šä¼ ç ”ç©¶èƒŒæ™¯æ–‡ä»¶
    5. ç‚¹å‡»"å¼€å§‹å†™ä½œ"å³å¯
    """)

# ä¸»ç•Œé¢ - é€‰é¡¹å¡
tab1, tab2, tab3, tab4 = st.tabs(
    ["ğŸ“Š é£æ ¼åˆ†æ", "ğŸ“š æ–‡çŒ®å¯¼å…¥", "âœï¸ ä¸€é”®å†™ä½œ", "ğŸ’¡ äº†è§£æ›´å¤š"]
)

# ========== Tab 1: é£æ ¼åˆ†æ ==========
with tab1:
    st.header("æœŸåˆŠé£æ ¼åˆ†æ")
    st.markdown("åˆ†æèŒƒæ–‡æ ·æœ¬ï¼Œæå–ç›®æ ‡æœŸåˆŠçš„å†™ä½œé£æ ¼")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ“¤ ä¸Šä¼ è®ºæ–‡èŒƒæ–‡")
        uploaded_papers = st.file_uploader(
            "é€‰æ‹©è®ºæ–‡æ–‡ä»¶ï¼ˆæ”¯æŒPDF/Word/Markdown/TXTï¼‰",
            type=["pdf", "docx", "doc", "md", "txt"],
            accept_multiple_files=True,
            key="papers_uploader",
        )
        st.info(
            "ğŸ’¡ **æç¤º**ï¼šä¸Šä¼ çš„è®ºæ–‡èŒƒæ–‡ä¸ä½ çš„ç ”ç©¶ä¸»é¢˜è¶Šç›¸å…³ï¼Œåˆ†æç»“æœè¶Šå‡†ç¡®ï¼Œç”Ÿæˆçš„è®ºæ–‡é£æ ¼è¶Šè´´è¿‘ç›®æ ‡æœŸåˆŠ"
        )

        # åˆå§‹åŒ–session state
        if "temp_papers_dir" not in st.session_state:
            st.session_state.temp_papers_dir = None
        if "uploaded_papers" not in st.session_state:
            st.session_state.uploaded_papers = None

        # åˆå§‹åŒ–é£æ ¼åˆ†æé€‰é¡¹ï¼ˆç”¨äºå›ºå®šé€‰é¡¹åŠŸèƒ½ï¼‰
        if "style_analysis_options" not in st.session_state:
            st.session_state.style_analysis_options = {
                "selected_sections": [
                    "abstract",
                    "introduction",
                    "methods",
                    "results",
                    "discussion",
                    "conclusion",
                ],
                "fixed_options": [
                    "abstract",
                    "introduction",
                    "methods",
                    "results",
                    "discussion",
                    "conclusion",
                ],
            }

        # å®šä¹‰æ‰€æœ‰å¯ç”¨çš„ç« èŠ‚é€‰é¡¹
        ALL_SECTION_OPTIONS = [
            {"value": "abstract", "label": "ğŸ“„ æ‘˜è¦ (Abstract)"},
            {"value": "introduction", "label": "ğŸ“ å¼•è¨€ (Introduction)"},
            {"value": "methods", "label": "ğŸ”¬ æ–¹æ³• (Methods)"},
            {"value": "results", "label": "ğŸ“Š ç»“æœ (Results)"},
            {"value": "discussion", "label": "ğŸ’¬ è®¨è®º (Discussion)"},
            {"value": "conclusion", "label": "âœ¨ ç»“è®º (Conclusion)"},
        ]

        # å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–°æ–‡ä»¶ï¼Œæ›´æ–°session stateå¹¶ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
        if uploaded_papers:
            st.session_state.uploaded_papers = uploaded_papers
            # é‡æ–°åˆ›å»ºä¸´æ—¶ç›®å½•ï¼ˆç¡®ä¿æ¯æ¬¡éƒ½å†™å…¥æ–‡ä»¶ï¼‰
            if st.session_state.temp_papers_dir and os.path.exists(
                st.session_state.temp_papers_dir
            ):
                shutil.rmtree(st.session_state.temp_papers_dir)
            st.session_state.temp_papers_dir = tempfile.mkdtemp()
            file_count = 0
            for f in st.session_state.uploaded_papers:
                try:
                    file_path = os.path.join(st.session_state.temp_papers_dir, f.name)
                    with open(file_path, "wb") as file:
                        file.write(f.getbuffer())
                    file_count += 1
                    st.write(f"âœ… å·²ä¿å­˜: {f.name}")
                except Exception as e:
                    st.error(f"âŒ ä¿å­˜å¤±è´¥ {f.name}: {str(e)}")
            st.success(f"å·²ä¸Šä¼  {file_count} ä¸ªæ–‡ä»¶")
        else:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå½“å‰çŠ¶æ€
            if st.session_state.uploaded_papers:
                st.info(
                    f"å·²é€‰æ‹© {len(st.session_state.uploaded_papers)} ä¸ªæ–‡ä»¶ï¼ˆæ¥è‡ªsessionï¼‰"
                )

    with col2:
        st.subheader("ğŸ“‚ æˆ–ä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
        use_local = st.checkbox("ä½¿ç”¨æœ¬åœ°èŒƒæ–‡", value=True, key="use_local_papers_2")
        local_dir = "input/sample_papers"
        if use_local:
            local_dir = st.text_input(
                "èŒƒæ–‡ç›®å½•", value="input/sample_papers", key="local_dir_input"
            )

        journal_name = st.text_input(
            "ç›®æ ‡æœŸåˆŠ", value="Nature Communications", key="journal_name_input"
        )

        # AIå¢å¼ºåˆ†æé€‰é¡¹
        use_ai_enhancement = st.checkbox(
            "ğŸ¤– ä½¿ç”¨AIå¢å¼ºåˆ†æ",
            value=True,  # æ”¹ä¸ºé»˜è®¤å¼€å¯
            help="ä½¿ç”¨DeepSeek AIè¿›è¡Œ8ç»´åº¦æ·±åº¦é£æ ¼åˆ†æï¼ˆéœ€è¦é…ç½®DeepSeek APIï¼‰",
            key="use_ai_enhancement",
        )

        if use_ai_enhancement:
            if not deepseek_api_key:
                st.warning("âš ï¸ ä½¿ç”¨AIå¢å¼ºåˆ†æéœ€è¦é…ç½®DeepSeek API Key")
            else:
                st.info(
                    "âœ… å°†ä½¿ç”¨AIå¢å¼ºåˆ†æï¼ŒåŸºäºjournal_section_style_skill.mdçš„8ç»´åº¦æ¡†æ¶"
                )

    # æ ¹æ®ç”¨æˆ·é€‰æ‹©ç¡®å®šä½¿ç”¨çš„ç›®å½•
    if st.session_state.uploaded_papers:
        papers_dir = st.session_state.temp_papers_dir
    elif use_local:
        papers_dir = local_dir
    else:
        papers_dir = None

    if st.button("ğŸ” åˆ†æé£æ ¼", type="primary"):
        if (
            not papers_dir
            or not os.path.exists(papers_dir)
            or not os.listdir(papers_dir)
        ):
            st.warning("è¯·ä¸Šä¼ æ–‡ä»¶æˆ–ç¡®ä¿æœ¬åœ°ç›®å½•æœ‰æ–‡ä»¶")
        else:
            # åˆ†æè¿›åº¦æ¡å’ŒçŠ¶æ€
            progress_bar = st.progress(0)
            status_text = st.empty()

            with st.spinner("ğŸ” æ­£åœ¨åˆ†ææœŸåˆŠé£æ ¼..."):
                try:
                    status_text.text("æ­£åœ¨åˆå§‹åŒ–åˆ†æå™¨...")
                    progress_bar.progress(10)

                    status_text.text("æ­£åœ¨å¤„ç†è®ºæ–‡æ–‡æœ¬...")
                    progress_bar.progress(30)

                    status_text.text("æ­£åœ¨åˆ†æè¯æ±‡ç‰¹å¾...")
                    progress_bar.progress(50)

                    status_text.text("æ­£åœ¨ç”Ÿæˆå†™ä½œæŒ‡å—...")
                    progress_bar.progress(80)

                    # æ‰§è¡Œå®é™…åˆ†æ
                    if use_ai_enhancement and deepseek_api_key:
                        # ä½¿ç”¨AIå¢å¼ºåˆ†æ
                        skill_file_path = r"E:\AI_projects\å­¦æœ¯å†™ä½œ\paper_writer\journal_section_style_skill.md"
                        if os.path.exists(skill_file_path):
                            result = analyze_journal_style_with_ai(
                                papers_dir,
                                "output/style",
                                journal_name,
                                deepseek_api_key,
                            )
                            st.info("âœ… ä½¿ç”¨AIå¢å¼ºåˆ†æå®Œæˆï¼ˆä¸¥æ ¼æŒ‰ç…§skill 8ç»´åº¦æ¡†æ¶ï¼‰")
                        else:
                            st.warning("âš ï¸ skillæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿåˆ†æ")
                            result = analyze_journal_style(
                                papers_dir, "output/style", journal_name
                            )
                    else:
                        # ä½¿ç”¨ä¼ ç»Ÿåˆ†æ
                        result = analyze_journal_style(
                            papers_dir, "output/style", journal_name
                        )

                    status_text.text("å®Œæˆï¼")
                    progress_bar.progress(100)

                    st.success("âœ… åˆ†æå®Œæˆ!")

                    # æ˜¾ç¤ºç»“æœæ‘˜è¦
                    st.subheader("ğŸ“Š åˆ†æç»“æœ")

                    # è¯»å–å¹¶æ˜¾ç¤ºé£æ ¼æ‘˜è¦
                    if os.path.exists(result["summary"]):
                        with open(result["summary"], "r", encoding="utf-8") as f:
                            summary_content = f.read()
                        st.markdown(summary_content)

                    # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
                    st.subheader("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶")

                    col1, col2 = st.columns(2)

                    with col1:
                        st.write("**æ ¸å¿ƒæ–‡ä»¶**")
                        if os.path.exists(result["report"]):
                            st.write(
                                f"ğŸ“„ å®Œæ•´æŠ¥å‘Š: {os.path.basename(result['report'])}"
                            )
                        if os.path.exists(result["summary"]):
                            st.write(
                                f"ğŸ“„ é£æ ¼æ‘˜è¦: {os.path.basename(result['summary'])}"
                            )

                    with col2:
                        st.write("**å†™ä½œæŒ‡å—**")
                        if "guides" in result and isinstance(result["guides"], dict):
                            for section, guide_path in result["guides"].items():
                                if os.path.exists(guide_path):
                                    st.write(
                                        f"ğŸ“„ {section.title()}æŒ‡å—: {os.path.basename(guide_path)}"
                                    )

                    # æ·»åŠ ä¸‹è½½æŒ‰é’®
                    st.subheader("ğŸ“¥ ä¸‹è½½ç»“æœ")

                    # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
                    style_dir = "output/style"
                    if os.path.exists(style_dir) and os.listdir(style_dir):
                        import zipfile
                        import io

                        try:
                            zip_buffer = io.BytesIO()
                            with zipfile.ZipFile(
                                zip_buffer, "w", zipfile.ZIP_DEFLATED
                            ) as zipf:
                                for root, dirs, files in os.walk(style_dir):
                                    for file in files:
                                        file_path = os.path.join(root, file)
                                        arcname = os.path.relpath(file_path, style_dir)
                                        zipf.write(file_path, arcname)

                            zip_buffer.seek(0)
                            zip_data = zip_buffer.getvalue()

                            # ä½¿ç”¨ Streamlit çš„ä¸‹è½½æŒ‰é’®
                            st.download_button(
                                label="ğŸ“¦ ä¸‹è½½æ‰€æœ‰åˆ†æç»“æœ (ZIP)",
                                data=zip_data,
                                file_name="style_analysis.zip",
                                mime="application/zip",
                                help="ä¸‹è½½åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å‹ç¼©åŒ…",
                            )
                            st.success("âœ… å‹ç¼©åŒ…å·²å‡†å¤‡å°±ç»ªï¼Œç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ä¸‹è½½")

                        except Exception as e:
                            st.error(f"åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {str(e)}")
                    else:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶ï¼Œæ— æ³•åˆ›å»ºä¸‹è½½åŒ…")

                    # æ˜¾ç¤ºè¯¦ç»†çš„æ–‡ä»¶è·¯å¾„
                    with st.expander("ğŸ“‚ å®Œæ•´æ–‡ä»¶è·¯å¾„"):
                        for key, value in result.items():
                            if isinstance(value, dict):
                                for sub_key, sub_path in value.items():
                                    if os.path.exists(sub_path):
                                        st.write(f"ğŸ“„ {key}/{sub_key}: {sub_path}")
                            else:
                                if os.path.exists(value):
                                    st.write(f"ğŸ“„ {key}: {value}")

                except Exception as e:
                    st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                    st.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ - åªåœ¨ç”¨æˆ·ç‚¹å‡»æ—¶æ¸…ç†
    if st.session_state.temp_papers_dir and os.path.exists(
        st.session_state.temp_papers_dir
    ):
        if st.button("ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶"):
            try:
                shutil.rmtree(st.session_state.temp_papers_dir)
                st.session_state.temp_papers_dir = None
                st.session_state.uploaded_papers = None
                st.success("ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
            except Exception as e:
                st.error(f"æ¸…ç†å¤±è´¥: {str(e)}")

# ========== Tab 2: æ–‡çŒ®å¯¼å…¥ ==========
with tab2:
    st.header("ğŸ“š æ–‡çŒ®æ•°æ®åº“ç®¡ç†")
    st.markdown("å¯¼å…¥Web of Scienceå¯¼å‡ºçš„Plain Textæ–‡çŒ®æ•°æ®")

    # ========== ä¸Šä¼ æ–‡çŒ®æ–‡ä»¶ ==========
    st.subheader("ğŸ“¤ ä¸Šä¼ WOSæ–‡çŒ®æ–‡ä»¶")
    st.markdown("ä¸Šä¼ Web of Scienceå¯¼å‡ºçš„ **Plain Text / Full Record** æ ¼å¼")
    st.caption("æ”¯æŒæ‰¹é‡ä¸Šä¼ å¤šä¸ª.txtæ–‡ä»¶")

    uploaded_literature_list = st.file_uploader(
        "é€‰æ‹©æ–‡çŒ®æ–‡ä»¶",
        type=["txt"],
        accept_multiple_files=True,
    )

    st.info(
        "ğŸ’¡ **æç¤º**ï¼šå¯¼å‡ºæ—¶é€‰æ‹© **Full Record** æ ¼å¼ï¼Œç¡®ä¿åŒ…å«æ‘˜è¦(AB)å’Œä½œè€…(AU)å­—æ®µã€‚"
    )

    # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
    txt_files_info = []  # å­˜å‚¨ (æ–‡ä»¶å, ä¸´æ—¶è·¯å¾„)
    temp_lit_dir = None

    if uploaded_literature_list:
        temp_lit_dir = tempfile.mkdtemp()
        file_count = 0
        for uploaded_file in uploaded_literature_list:
            path = os.path.join(temp_lit_dir, uploaded_file.name)
            with open(path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            txt_files_info.append((uploaded_file.name, path))
            file_count += 1
        st.success(f"å·²ä¸Šä¼  {file_count} ä¸ªæ–‡ä»¶")

    # å¯¼å…¥æŒ‰é’®ï¼ˆæ‰¹é‡å¯¼å…¥ï¼‰
    if txt_files_info:
        if st.button("ğŸ“¥ å¯¼å…¥æ‰€æœ‰æ–‡çŒ®æ–‡ä»¶", type="primary"):
            with st.spinner("å¯¼å…¥ä¸­..."):
                imported_count = 0
                error_count = 0
                for file_name, file_path in txt_files_info:
                    # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“
                    safe_name = (
                        file_name.replace(" ", "_")
                        .replace("-", "_")
                        .replace(".txt", "")
                    )
                    db_path = f"output/{safe_name}.db"

                    try:
                        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹
                        if not os.path.exists(file_path):
                            st.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_name}")
                            error_count += 1
                            continue

                        file_size = os.path.getsize(file_path)
                        if file_size == 0:
                            st.error(f"âŒ æ–‡ä»¶ä¸ºç©º: {file_name}")
                            error_count += 1
                            continue

                        # å¯¼å…¥åˆ°æ•°æ®åº“
                        manager = create_literature_database(file_path, db_path)
                        stats = manager.get_statistics()

                        if stats["total_papers"] > 0:
                            imported_count += 1
                            st.success(
                                f"âœ… {file_name}: {stats['total_papers']} ç¯‡æ–‡çŒ®"
                            )
                        else:
                            st.warning(
                                f"âš ï¸ {file_name}: 0 ç¯‡æ–‡çŒ®ï¼ˆæ–‡ä»¶å¯èƒ½ä¸ºç©ºæˆ–æ ¼å¼ä¸å¯¹ï¼‰"
                            )
                            error_count += 1

                    except Exception as e:
                        st.error(f"âŒ å¯¼å…¥å¤±è´¥: {file_name} - {str(e)}")
                        error_count += 1

                # æ˜¾ç¤ºæ€»ç»“
                if imported_count > 0:
                    st.success(
                        f"âœ… æˆåŠŸå¯¼å…¥ {imported_count}/{len(txt_files_info)} ä¸ªæ–‡ä»¶"
                    )
                    st.rerun()
                else:
                    st.error(
                        f"âŒ å¯¼å…¥å¤±è´¥ï¼Œæ‰€æœ‰ {len(txt_files_info)} ä¸ªæ–‡ä»¶éƒ½å¯¼å…¥å¤±è´¥"
                    )

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if uploaded_literature_list and temp_lit_dir and os.path.exists(temp_lit_dir):
        try:
            shutil.rmtree(temp_lit_dir)
        except:
            pass

    st.divider()

    # ========== é€‰æ‹©è¦æŸ¥çœ‹çš„æ–‡çŒ®æ–‡ä»¶ ==========
    st.subheader("ğŸ“‚ é€‰æ‹©è¦æŸ¥çœ‹çš„æ–‡çŒ®æ–‡ä»¶")

    # è·å–æ‰€æœ‰å·²å¯¼å…¥çš„æ•°æ®åº“æ–‡ä»¶
    output_dir = "output"
    if os.path.exists(output_dir):
        db_files = [
            f.replace(".db", "") for f in os.listdir(output_dir) if f.endswith(".db")
        ]
    else:
        db_files = []

    if not db_files:
        st.info("æš‚æ— å·²å¯¼å…¥çš„æ–‡çŒ®åº“ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¯¼å…¥æ–‡çŒ®æ–‡ä»¶")
    else:
        # ä¸‹æ‹‰èœå•é€‰æ‹© + åˆ é™¤æŒ‰é’®
        col1, col2 = st.columns([3, 1])
        with col1:
            selected_db_name = st.selectbox(
                "é€‰æ‹©æ–‡çŒ®æ–‡ä»¶",
                options=db_files,
                key="file_selector",
            )
        with col2:
            st.write("")  # å ä½å¯¹é½
            st.write("")  # å ä½å¯¹é½
            if st.button("ğŸ—‘ï¸ åˆ é™¤", type="secondary", use_container_width=True):
                db_path = f"output/{selected_db_name}.db"
                if os.path.exists(db_path):
                    os.remove(db_path)
                    st.success("å·²åˆ é™¤")
                    st.rerun()

        # æ•°æ®åº“è·¯å¾„
        db_path = f"output/{selected_db_name}.db"

        if os.path.exists(db_path):
            st.success(f"å·²åŠ è½½: {selected_db_name}")

        st.divider()

        # ========== ä¸‰ä¸ªä¸‹è½½å¡ç‰‡ ==========
        st.subheader("ğŸ“¦ å¯¼å‡ºæ–‡ä»¶")

        has_data = False
        jsonl_content = ""
        bibtex_content = ""
        excel_buffer = io.BytesIO()
        jsonl_count = 0
        bib_count = 0
        excel_count = 0

        if os.path.exists(db_path):
            try:
                manager = LiteratureDatabaseManager(db_path)
                stats = manager.get_statistics()

                if stats["total_papers"] > 0:
                    has_data = True
                    papers = manager.search("", limit=10000)

                    # JSONLæ•°æ®
                    jsonl_lines = []
                    for paper in papers:
                        first_author = ""
                        if paper.authors:
                            first_author = (
                                paper.authors.split(",")[0].strip().split()[-1]
                            )
                        citation_text = (
                            f"({first_author} et al., {paper.year})"
                            if first_author and paper.year
                            else ""
                        )

                        jsonl_lines.append(
                            json.dumps(
                                {
                                    "paper_id": paper.wos_id or f"id_{paper.id}",
                                    "title": paper.title,
                                    "year": paper.year,
                                    "authors": paper.authors.split("; ")
                                    if paper.authors
                                    else [],
                                    "first_author_lastname": first_author,
                                    "doi": paper.doi,
                                    "abstract": paper.abstract,
                                    "citation_text": citation_text,
                                    "source": "Web of Science",
                                },
                                ensure_ascii=False,
                            )
                        )
                    jsonl_content = "\n".join(jsonl_lines)
                    jsonl_count = len(jsonl_lines)

                    # BibTeXæ•°æ®
                    bibtex_content = manager.export_to_bibtex()
                    bib_count = bibtex_content.count("@article")

                    # Excelæ•°æ®
                    papers_data = []
                    for paper in papers:
                        papers_data.append(
                            {
                                "Authors": paper.authors,
                                "Title": paper.title,
                                "Journal": paper.journal,
                                "Year": paper.year,
                                "Volume": paper.volume,
                                "DOI": paper.doi,
                                "Abstract": paper.abstract,
                                "Cited By": paper.cited_by,
                            }
                        )
                    excel_buffer = io.BytesIO()
                    pd.DataFrame(papers_data).to_excel(
                        excel_buffer, index=False, engine="openpyxl"
                    )
                    excel_buffer.seek(0)
                    excel_count = len(papers_data)
            except Exception as e:
                st.warning(f"åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")

        # æ˜¾ç¤ºä¸‰ä¸ªå¡ç‰‡
        col_card1, col_card2, col_card3 = st.columns(3)

        # å¡ç‰‡1: JSONL
        with col_card1:
            with st.container(border=True):
                st.markdown(
                    """
                <div style="text-align: center; padding: 10px;">
                    <span style="font-size: 40px;">ğŸ“„</span>
                    <h4 style="margin: 10px 0;">JSONL</h4>
                    <p style="color: gray; font-size: 12px;">ç»“æ„åŒ–æ–‡çŒ®æ•°æ®</p>
                </div>
                """,
                    unsafe_allow_html=True,
                )
                if has_data:
                    st.write(f"ğŸ“Š å…± **{jsonl_count}** æ¡è®°å½•")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½JSONL",
                        data=jsonl_content,
                        file_name=f"{selected_db_name}.jsonl",
                        mime="application/json",
                        use_container_width=True,
                    )
                else:
                    st.write("ğŸ“Š å…± **0** æ¡è®°å½•")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½JSONL",
                        data="",
                        file_name="literature.jsonl",
                        mime="application/json",
                        use_container_width=True,
                        disabled=True,
                    )

        # å¡ç‰‡2: BibTeX
        with col_card2:
            with st.container(border=True):
                st.markdown(
                    """
                <div style="text-align: center; padding: 10px;">
                    <span style="font-size: 40px;">ğŸ“š</span>
                    <h4 style="margin: 10px 0;">BibTeX</h4>
                    <p style="color: gray; font-size: 12px;">å‚è€ƒæ–‡çŒ®æ ¼å¼</p>
                </div>
                """,
                    unsafe_allow_html=True,
                )
                if has_data:
                    st.write(f"ğŸ“Š å…± **{bib_count}** æ¡å¼•ç”¨")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½BibTeX",
                        data=bibtex_content,
                        file_name=f"{selected_db_name}.bib",
                        mime="text/plain",
                        use_container_width=True,
                    )
                else:
                    st.write("ğŸ“Š å…± **0** æ¡å¼•ç”¨")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½BibTeX",
                        data="",
                        file_name="literature.bib",
                        mime="text/plain",
                        use_container_width=True,
                        disabled=True,
                    )

        # å¡ç‰‡3: Excel
        with col_card3:
            with st.container(border=True):
                st.markdown(
                    """
                <div style="text-align: center; padding: 10px;">
                    <span style="font-size: 40px;">ğŸ“Š</span>
                    <h4 style="margin: 10px 0;">Excel</h4>
                    <p style="color: gray; font-size: 12px;">è¡¨æ ¼æ•°æ®</p>
                </div>
                """,
                    unsafe_allow_html=True,
                )
                if has_data:
                    st.write(f"ğŸ“Š å…± **{excel_count}** æ¡è®°å½•")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½Excel",
                        data=excel_buffer,
                        file_name=f"{selected_db_name}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True,
                    )
                else:
                    st.write("ğŸ“Š å…± **0** æ¡è®°å½•")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½Excel",
                        data=b"",
                        file_name="literature.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True,
                        disabled=True,
                    )

        st.divider()

        # ========== çŸ¥è¯†åº“å†…å®¹ç®¡ç† ==========
        st.subheader("ğŸ“š çŸ¥è¯†åº“å†…å®¹ç®¡ç†")

        # åˆå§‹åŒ–session state
        if "selected_papers_to_delete" not in st.session_state:
            st.session_state.selected_papers_to_delete = set()

        try:
            if os.path.exists(db_path):
                manager = LiteratureDatabaseManager(db_path)
                stats = manager.get_statistics()

                if stats["total_papers"] > 0:
                    # ç»Ÿè®¡ä¿¡æ¯
                    col_count1, col_count2, col_count3 = st.columns(3)
                    with col_count1:
                        st.metric("æ–‡çŒ®æ€»æ•°", stats["total_papers"])
                    with col_count2:
                        if stats.get("year_distribution"):
                            years = [
                                y
                                for y in stats["year_distribution"].keys()
                                if isinstance(y, int)
                            ]
                            if years:
                                st.metric("å¹´ä»½èŒƒå›´", f"{min(years)} - {max(years)}")
                            else:
                                st.metric("å¹´ä»½èŒƒå›´", "N/A")
                        else:
                            st.metric("å¹´ä»½èŒƒå›´", "N/A")
                    with col_count3:
                        if stats.get("top_journals"):
                            top_j = sorted(
                                stats["top_journals"].items(),
                                key=lambda x: x[1],
                                reverse=True,
                            )[:3]
                            st.write(
                                "TopæœŸåˆŠ: "
                                + ", ".join(
                                    [
                                        f"{j[:15]}..." if len(j) > 15 else j
                                        for j, c in top_j
                                    ]
                                )
                            )
                        else:
                            st.write("TopæœŸåˆŠ: æ— ")

                    # æœç´¢å’Œç­›é€‰
                    col_search1, col_search2 = st.columns([3, 1])
                    with col_search1:
                        search_query = st.text_input(
                            "ğŸ” æœç´¢æ–‡çŒ®",
                            placeholder="è¾“å…¥å…³é”®è¯æœç´¢æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦...",
                        )
                    with col_search2:
                        all_years = sorted(
                            [
                                str(y)
                                for y in stats["year_distribution"].keys()
                                if isinstance(y, int)
                            ],
                            reverse=True,
                        )
                        year_filter = st.selectbox("å¹´ä»½ç­›é€‰", ["å…¨éƒ¨"] + all_years)

                    # è·å–æ–‡çŒ®åˆ—è¡¨
                    papers = (
                        manager.search(search_query, limit=1000)
                        if search_query
                        else manager.search("", limit=1000)
                    )
                    papers = [p for p in papers if p.year > 0]

                    if year_filter != "å…¨éƒ¨":
                        papers = [p for p in papers if p.year == int(year_filter)]

                    st.write(f"å…±æ‰¾åˆ° **{len(papers)}** ç¯‡æ–‡çŒ®")

                    # ç›´æ¥æ˜¾ç¤ºæ–‡çŒ®å†…å®¹ï¼Œä¸éœ€è¦æŠ˜å 
                    st.subheader("ğŸ“‹ æŸ¥çœ‹æ–‡çŒ®å†…å®¹")

                    if len(papers) > 0:
                        # å‡†å¤‡æ•°æ®
                        papers_df = pd.DataFrame(
                            [
                                {
                                    "æ ‡é¢˜": paper.title,
                                    "ä½œè€…": (paper.authors[:50] + "...")
                                    if paper.authors and len(paper.authors) > 50
                                    else (paper.authors or ""),
                                    "å¹´ä»½": paper.year,
                                    "æœŸåˆŠ": (paper.journal[:30] + "...")
                                    if paper.journal and len(paper.journal) > 30
                                    else (paper.journal or ""),
                                }
                                for paper in papers
                            ]
                        )

                        # æ˜¾ç¤ºè¡¨æ ¼
                        st.dataframe(
                            papers_df,
                            hide_index=True,
                            use_container_width=True,
                        )
                    else:
                        st.info("æš‚æ— æ–‡çŒ®")

                    st.divider()

                    # ç»Ÿè®¡ä¿¡æ¯
                    st.subheader("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
                    st.write(f"å½“å‰æ–‡çŒ®åº“å…± **{len(papers)}** ç¯‡æ–‡çŒ®")

                else:
                    st.info("å½“å‰æ–‡çŒ®åº“ä¸ºç©º")
            else:
                st.info("æ–‡çŒ®åº“ä¸å­˜åœ¨")

        except Exception as e:
            st.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if uploaded_literature_list and temp_lit_dir and os.path.exists(temp_lit_dir):
        try:
            shutil.rmtree(temp_lit_dir)
        except:
            pass

# ========== Tab 3: ä¸€é”®å†™ä½œï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰ ==========
with tab3:
    st.header("âœï¸ ä¸€é”®å†™ä½œ")
    st.markdown(
        "ä¸Šä¼ ç ”ç©¶èƒŒæ™¯æ–‡æ¡£ï¼ˆæ”¯æŒWordï¼‰ï¼Œç³»ç»Ÿè‡ªåŠ¨è§£ææ–‡æœ¬ã€å›¾è¡¨å’Œè¡¨æ ¼ï¼ŒAIåˆ†æåä¾›æ‚¨ç¡®è®¤ï¼Œæœ€åæ’°å†™å®Œæ•´è®ºæ–‡"
    )

    # Initialize session state for document analysis
    if "doc_analysis_result" not in st.session_state:
        st.session_state.doc_analysis_result = None
    if "corrected_content" not in st.session_state:
        st.session_state.corrected_content = None

    col1, col2 = st.columns([1.3, 1])

    with col1:
        st.subheader("ğŸ“¤ ä¸Šä¼ ç ”ç©¶æ–‡æ¡£ï¼ˆWordæ ¼å¼æ¨èï¼‰")
        st.markdown("ä¸Šä¼ åŒ…å«ç ”ç©¶èƒŒæ™¯ã€æ•°æ®ã€å›¾è¡¨çš„Wordæ–‡æ¡£")

        uploaded_doc = st.file_uploader(
            "é€‰æ‹©Wordæ–‡æ¡£ï¼ˆ.docxï¼‰",
            type=["docx"],
            help="Wordæ–‡æ¡£å¯ä»¥åŒ…å«æ–‡å­—ã€è¡¨æ ¼å’Œæˆªå›¾å›¾è¡¨ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è§£æ",
            key="doc_uploader_tab3",
        )

        if uploaded_doc:
            st.success(f"å·²ä¸Šä¼ : {uploaded_doc.name}")

            # Save to temp file for analysis
            temp_doc_dir = tempfile.mkdtemp()
            doc_path = os.path.join(temp_doc_dir, uploaded_doc.name)
            with open(doc_path, "wb") as f:
                f.write(uploaded_doc.getbuffer())

            # Analyze button
            col_btn1, col_btn2 = st.columns([1, 2])
            with col_btn1:
                analyze_btn = st.button(
                    "ğŸ” è§£ææ–‡æ¡£å¹¶åˆ†æå›¾è¡¨",
                    type="primary",
                    key="analyze_doc_btn",
                )

            # Progress indicator
            if "analyzing_doc" not in st.session_state:
                st.session_state.analyzing_doc = False

            if analyze_btn and not st.session_state.analyzing_doc:
                st.session_state.analyzing_doc = True
                st.rerun()

            # Display analysis results and allow review
            if st.session_state.get("analyzing_doc"):
                with st.spinner("æ­£åœ¨è§£ææ–‡æ¡£å’Œåˆ†æå›¾è¡¨..."):
                    try:
                        from document_processor.word_analyzer import (
                            WordDocumentAnalyzer,
                            CorrectedContent,
                        )
                        from coordinator.multi_agent_coordinator import APIClient

                        # Create API client if configured
                        api_client = None
                        if api_url and api_key:
                            api_client = APIClient(api_url, api_key)

                        # Analyze document
                        analyzer = WordDocumentAnalyzer(api_client)
                        result = analyzer.analyze_document(
                            doc_path, analyze_images=bool(api_client)
                        )

                        st.session_state.doc_analysis_result = result
                        st.session_state.corrected_content = CorrectedContent()
                        st.session_state.analyzing_doc = False

                        st.success("æ–‡æ¡£è§£æå®Œæˆï¼")

                    except Exception as e:
                        st.error(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
                        st.session_state.analyzing_doc = False

        # Document preview and confirmation (moved here)
        if st.session_state.doc_analysis_result:
            result = st.session_state.doc_analysis_result
            st.success("âœ“ æ–‡æ¡£å·²è§£æ")

            # Initialize corrected content if needed
            if st.session_state.corrected_content is None:
                from document_processor.word_analyzer import CorrectedContent

                st.session_state.corrected_content = CorrectedContent()

            corrected = st.session_state.corrected_content

            # Summary of extracted elements
            st.markdown(f"""
            **æ–‡æ¡£å†…å®¹æ‘˜è¦ï¼š**
            - æ–‡æœ¬æ®µè½: {len([e for e in result.text_elements if e.element_type in ["Title", "Paragraph"]])} ä¸ª
            - è¡¨æ ¼: {len(result.table_elements)} ä¸ª
            - å›¾ç‰‡/å›¾è¡¨: {len(result.image_elements)} ä¸ª
            """)

            # Extract text content for the paper
            text_parts = []
            for elem in result.text_elements:
                if elem.element_type == "Title":
                    text_parts.append(f"# {elem.text}")
                elif elem.element_type == "Table":
                    text_parts.append(f"\n[è¡¨æ ¼æ•°æ®]\n{elem.text}")
                else:
                    text_parts.append(elem.text)
            corrected.text_content = "\n\n".join(text_parts)

            # Build results data from tables and images
            results_parts = []

            # Add table descriptions
            if result.table_elements:
                results_parts.append("ã€è¡¨æ ¼åˆ†æç»“æœã€‘")
                for table in result.table_elements:
                    if api_url and api_key:
                        from coordinator.multi_agent_coordinator import APIClient
                        from document_processor.word_analyzer import (
                            WordDocumentAnalyzer,
                        )

                        api_client_inner = APIClient(api_url, api_key)
                        analyzer_inner = WordDocumentAnalyzer(api_client_inner)
                        description = analyzer_inner.analyze_table_with_ai(table)
                        results_parts.append(
                            f"\n{table.caption or table.table_id}:\n{description}"
                        )
                        corrected.table_descriptions[table.table_id] = description
                    else:
                        results_parts.append(
                            f"\n{table.caption or table.table_id}: (éœ€è¦APIé…ç½®æ‰èƒ½åˆ†æ)"
                        )
                        corrected.table_descriptions[table.table_id] = (
                            "(éœ€è¦APIé…ç½®æ‰èƒ½åˆ†æè¡¨æ ¼å†…å®¹)"
                        )

            # Add image descriptions
            if result.image_elements:
                results_parts.append("\nã€å›¾è¡¨åˆ†æç»“æœã€‘")
                for img in result.image_elements:
                    if api_url and api_key:
                        with open(img.image_path, "rb") as f:
                            import base64

                            image_data = base64.b64encode(f.read()).decode()

                        prompt = """åˆ†æè¿™å¼ å›¾è¡¨å›¾ç‰‡ï¼Œè¯·æä¾›ï¼š
1. å›¾è¡¨ç±»å‹ï¼ˆæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ç­‰ï¼‰
2. ä¸»è¦æ•°æ®è¶‹åŠ¿
3. å…³é”®æ•°å€¼æˆ–ç»Ÿè®¡ç»“æœ

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æè¿°ã€‚"""

                        messages = [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": prompt},
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{image_data}"
                                        },
                                    },
                                ],
                            }
                        ]

                        from coordinator.multi_agent_coordinator import APIClient

                        api_client_inner = APIClient(api_url, api_key)
                        try:
                            response = api_client_inner.call_model(
                                model="gpt-4o", messages=messages, max_tokens=1000
                            )
                            img.description = response
                            results_parts.append(
                                f"\n{img.caption or img.image_id}:\n{response}"
                            )
                        except Exception as e:
                            img.description = f"åˆ†æå¤±è´¥: {str(e)}"
                            results_parts.append(
                                f"\n{img.caption or img.image_id}: åˆ†æå¤±è´¥"
                            )
                    else:
                        results_parts.append(
                            f"\n{img.caption or img.image_id}: (éœ€è¦APIé…ç½®æ‰èƒ½åˆ†æ)"
                        )
                        img.description = "ï¼ˆéœ€è¦é…ç½®APIæ‰èƒ½åˆ†æå›¾è¡¨ï¼‰"

            corrected.results_data = "\n".join(results_parts)

            # Show preview of confirmed data
            with st.expander("ğŸ“„ æŸ¥çœ‹å·²ç¡®è®¤çš„ç ”ç©¶æ•°æ®", expanded=True):
                st.markdown(corrected.results_data)

    with col2:
        st.subheader("ğŸ“‹ ä¸Šä¼ æ–‡æ¡£è¯´æ˜")

        st.info("""
        **è¯·å°†æ‰€æœ‰ç ”ç©¶å†…å®¹æ‰“åŒ…åˆ°ä¸€ä¸ªWordæ–‡æ¡£ä¸­ä¸Šä¼ **
        """)

        # Download template button
        template_path = "research_content_template.md"
        if os.path.exists(template_path):
            with open(template_path, "r", encoding="utf-8") as f:
                template_content = f.read()
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ç ”ç©¶å†…å®¹æ¨¡æ¿",
                data=template_content,
                file_name="research_content_template.md",
                mime="text/markdown",
                help="ä¸‹è½½æ¨¡æ¿æ–‡æ¡£ï¼ŒæŒ‰ç…§æ¨¡æ¿ç»“æ„æ•´ç†æ‚¨çš„ç ”ç©¶å†…å®¹",
                key="download_template",
            )
        else:
            st.caption("æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°")

        st.markdown("""
        **æ–‡æ¡£åº”åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š**

        1. **ç ”ç©¶èƒŒæ™¯** - ä¸»é¢˜ã€æ„ä¹‰ã€ç ”ç©¶ç°çŠ¶
        2. **è¯•éªŒè®¾è®¡** - ç›®çš„ã€å‡è®¾ã€åˆ†ç»„ã€æ ·æœ¬é‡
        3. **è¯•éªŒæ–¹æ³•** - æ•°æ®æ”¶é›†ã€ä»ªå™¨ã€ç»Ÿè®¡æ–¹æ³•
        4. **è¯•éªŒåœ°ç‚¹ä¸ç¯å¢ƒ** - åœ°ç‚¹ã€æ¡ä»¶ã€å‘¨æœŸ
        5. **ç ”ç©¶ç»“æœ** - æ•°æ®è¡¨æ ¼ã€å›¾è¡¨

        ğŸ’¡ **æç¤º**ï¼šæ‰€æœ‰å†…å®¹æ”¾å…¥ä¸€ä¸ªWordæ–‡æ¡£
        """)

        # Display document analysis status
        if st.session_state.doc_analysis_result:
            result = st.session_state.doc_analysis_result
            st.success("âœ“ æ–‡æ¡£è§£æå®Œæˆ")

            # Initialize corrected content if needed
            if st.session_state.corrected_content is None:
                from document_processor.word_analyzer import CorrectedContent

                st.session_state.corrected_content = CorrectedContent()

            corrected = st.session_state.corrected_content

            # Summary of extracted elements
            st.markdown(f"""
            **æ–‡æ¡£å†…å®¹æ‘˜è¦ï¼š**
            - æ–‡æœ¬æ®µè½: {len([e for e in result.text_elements if e.element_type in ["Title", "Paragraph"]])} ä¸ª
            - è¡¨æ ¼: {len(result.table_elements)} ä¸ª
            - å›¾ç‰‡/å›¾è¡¨: {len(result.image_elements)} ä¸ª
            """)

            # Extract text content for the paper
            text_parts = []
            for elem in result.text_elements:
                if elem.element_type == "Title":
                    text_parts.append(f"# {elem.text}")
                elif elem.element_type == "Table":
                    text_parts.append(f"\n[è¡¨æ ¼æ•°æ®]\n{elem.text}")
                else:
                    text_parts.append(elem.text)
            corrected.text_content = "\n\n".join(text_parts)

            # Build results data from tables and images
            results_parts = []

            # Add table descriptions
            if result.table_elements:
                results_parts.append("ã€è¡¨æ ¼åˆ†æç»“æœã€‘")
                for table in result.table_elements:
                    if api_url and api_key:
                        from coordinator.multi_agent_coordinator import APIClient
                        from document_processor.word_analyzer import (
                            WordDocumentAnalyzer,
                        )

                        api_client_inner = APIClient(api_url, api_key)
                        analyzer_inner = WordDocumentAnalyzer(api_client_inner)
                        description = analyzer_inner.analyze_table_with_ai(table)
                        results_parts.append(
                            f"\n{table.caption or table.table_id}:\n{description}"
                        )
                        corrected.table_descriptions[table.table_id] = description
                    else:
                        results_parts.append(
                            f"\n{table.caption or table.table_id}: (éœ€è¦APIé…ç½®æ‰èƒ½åˆ†æ)"
                        )
                        corrected.table_descriptions[table.table_id] = (
                            "(éœ€è¦APIé…ç½®æ‰èƒ½åˆ†æè¡¨æ ¼å†…å®¹)"
                        )

            # Add image descriptions
            if result.image_elements:
                results_parts.append("\nã€å›¾è¡¨åˆ†æç»“æœã€‘")
                for img in result.image_elements:
                    if api_url and api_key:
                        with open(img.image_path, "rb") as f:
                            import base64

                            image_data = base64.b64encode(f.read()).decode()

                        prompt = """åˆ†æè¿™å¼ å›¾è¡¨å›¾ç‰‡ï¼Œè¯·æä¾›ï¼š
1. å›¾è¡¨ç±»å‹ï¼ˆæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€é¥¼å›¾ã€çƒ­åŠ›å›¾ç­‰ï¼‰
2. å›¾è¡¨æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾
3. ä¸»è¦æ•°æ®è¶‹åŠ¿å’Œå‘ç°
4. å…³é”®æ•°å€¼æˆ–ç»Ÿè®¡ç»“æœ
5. å›¾è¡¨çš„å®Œæ•´æè¿°

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æè¿°ã€‚"""

                        messages = [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": prompt},
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{image_data}"
                                        },
                                    },
                                ],
                            }
                        ]

                        from coordinator.multi_agent_coordinator import APIClient

                        api_client_inner = APIClient(api_url, api_key)
                        try:
                            response = api_client_inner.call_model(
                                model="gpt-4o", messages=messages, max_tokens=1000
                            )
                            img.description = response
                            results_parts.append(
                                f"\n{img.caption or img.image_id}:\n{response}"
                            )
                        except Exception as e:
                            img.description = f"åˆ†æå¤±è´¥: {str(e)}"
                            results_parts.append(
                                f"\n{img.caption or img.image_id}: åˆ†æå¤±è´¥"
                            )
                    else:
                        results_parts.append(
                            f"\n{img.caption or img.image_id}: (éœ€è¦APIé…ç½®æ‰èƒ½åˆ†æ)"
                        )
                        img.description = "ï¼ˆéœ€è¦é…ç½®APIæ‰èƒ½åˆ†æå›¾è¡¨ï¼‰"

            corrected.results_data = "\n".join(results_parts)

            # Show preview of confirmed data
            with st.expander("æŸ¥çœ‹å·²ç¡®è®¤çš„ç ”ç©¶æ•°æ®", expanded=True):
                st.markdown(corrected.results_data)

        else:
            st.info("è¯·ä¸Šä¼ Wordæ–‡æ¡£å¹¶ç‚¹å‡»è§£æ")

    # Settings section (moved to bottom)
    st.divider()
    st.subheader("ğŸ“ è¾“å‡ºè®¾ç½®")

    col_settings1, col_settings2 = st.columns(2)

    with col_settings1:
        output_dir = st.text_input(
            "ğŸ“ è¾“å‡ºç›®å½•",
            value="output",
            key="output_dir_input_tab3",
            help="è®ºæ–‡è‰ç¨¿çš„è¾“å‡ºä¿å­˜ç›®å½•",
        )

    with col_settings2:
        if st.session_state.doc_analysis_result:
            st.success("âœ“ ç ”ç©¶æ•°æ®å·²å‡†å¤‡å°±ç»ª")
        journal = st.text_input(
            "ğŸ“š ç›®æ ‡æœŸåˆŠ",
            value="Nature Communications",
            help="é€‰æ‹©ç›®æ ‡æŠ•ç¨¿æœŸåˆŠï¼ŒAIå°†æ ¹æ®æœŸåˆŠé£æ ¼è°ƒæ•´è®ºæ–‡æ ¼å¼",
            key="journal_input_tab3_main",
        )

    # ä»»åŠ¡æ¸…å•å’Œæ¨¡å‹é€‰æ‹©ï¼ˆä½¿ç”¨å®Œæ•´å®½åº¦ï¼‰
    st.divider()
    col_task, col_model = st.columns([1.2, 1.5])

    with col_task:
        st.subheader("ğŸ“‹ ä»»åŠ¡æ¸…å•")
        st.info("ç³»ç»Ÿå°†è‡ªåŠ¨å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š")
        st.write("âœ… 1. åˆ†æèŒƒæ–‡é£æ ¼ï¼ˆå¦‚æœ‰ï¼‰")
        st.write("âœ… 2. å¯¼å…¥æ–‡çŒ®æ•°æ®åº“ï¼ˆå¦‚æœ‰ï¼‰")
        st.write("âœ… 3. å¤„ç†ç ”ç©¶æˆæœï¼ˆå¦‚æœ‰ï¼‰")
        st.write("âœ… 4. æ’°å†™å¼•è¨€ï¼ˆIntroductionï¼‰")
        st.write("âœ… 5. æ’°å†™æ–¹æ³•ï¼ˆMethodsï¼‰")
        st.write("âœ… 6. æ’°å†™ç»“æœï¼ˆResultsï¼‰")
        st.write("âœ… 7. æ’°å†™è®¨è®ºï¼ˆDiscussionï¼‰")
        st.write("âœ… 8. æ’°å†™æ‘˜è¦ï¼ˆAbstractï¼‰- ä¸€çº§AIå…¨å±€è§†è§’")
        st.write("âœ… 9. æ’°å†™ç»“è®ºï¼ˆConclusionï¼‰- ä¸€çº§AIå…¨å±€è§†è§’")
        st.write("âœ… 10. **è‡ªåŠ¨æ•´åˆ**æ‰€æœ‰ç« èŠ‚")
        st.write("âœ… 11. è´¨é‡æ£€æŸ¥ä¸ä¼˜åŒ–")

    with col_model:
        # Model selection for each section
        st.subheader("ğŸ¤– æ¨¡å‹é€‰æ‹©")
        st.markdown("ä¸ºæ¯ä¸ªç« èŠ‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹")

        # Define available models with descriptions
        available_models = [
            ("GPT-4o", "GPT-4o - Balanced"),
            ("GPT-4o-mini", "GPT-4o-mini - Fast/Economical"),
            (
                "Claude-Sonnet-4.5",
                "Claude-Sonnet-4.5 - Critical thinking",
            ),
            (
                "Claude-Opus-4.5",
                "Claude-Opus-4.5 - Highest quality",
            ),
            ("Claude-Sonnet-4", "Claude-Sonnet-4 - Strong reasoning"),
            ("deepseek-chat", "DeepSeek-V3 - Cost-effective"),
        ]

        # Default model recommendations from coordinator config
        default_models = {
            "introduction": "GPT-4o",
            "methods": "GPT-4o",
            "results": "GPT-4o",
            "discussion": "Claude-Sonnet-4.5",
            "abstract": "GPT-4o",
            "conclusion": "Claude-Sonnet-4.5",
        }

        # Create columns for model selection
        col_model1, col_model2 = st.columns(2)

        # Model selections
        with col_model1:
            model_intro = st.selectbox(
                "å¼•è¨€",
                options=[m[0] for m in available_models],
                format_func=lambda x: next(
                    (m[1] for m in available_models if m[0] == x), x
                ),
                index=[m[0] for m in available_models].index(
                    default_models["introduction"]
                ),
                key="model_intro",
            )
            model_methods = st.selectbox(
                "æ–¹æ³•",
                options=[m[0] for m in available_models],
                format_func=lambda x: next(
                    (m[1] for m in available_models if m[0] == x), x
                ),
                index=[m[0] for m in available_models].index(default_models["methods"]),
                key="model_methods",
            )
            model_results = st.selectbox(
                "ç»“æœ",
                options=[m[0] for m in available_models],
                format_func=lambda x: next(
                    (m[1] for m in available_models if m[0] == x), x
                ),
                index=[m[0] for m in available_models].index(default_models["results"]),
                key="model_results",
            )

        with col_model2:
            model_discussion = st.selectbox(
                "è®¨è®º",
                options=[m[0] for m in available_models],
                format_func=lambda x: next(
                    (m[1] for m in available_models if m[0] == x), x
                ),
                index=[m[0] for m in available_models].index(
                    default_models["discussion"]
                ),
                key="model_discussion",
            )
            model_abstract = st.selectbox(
                "æ‘˜è¦",
                options=[m[0] for m in available_models],
                format_func=lambda x: next(
                    (m[1] for m in available_models if m[0] == x), x
                ),
                index=[m[0] for m in available_models].index(
                    default_models["abstract"]
                ),
                key="model_abstract",
            )
            model_conclusion = st.selectbox(
                "ç»“è®º",
                options=[m[0] for m in available_models],
                format_func=lambda x: next(
                    (m[1] for m in available_models if m[0] == x), x
                ),
                index=[m[0] for m in available_models].index(
                    default_models["conclusion"]
                ),
                key="model_conclusion",
            )

        # Configuration summary
        st.caption(f"""
        **å½“å‰é…ç½®ï¼š**
        å¼•è¨€:{model_intro} | æ–¹æ³•:{model_methods} | ç»“æœ:{model_results} |
        è®¨è®º:{model_discussion} | æ‘˜è¦:{model_abstract} | ç»“è®º:{model_conclusion}
        """)

    # Build model config dictionary
    model_config = {
        "introduction": model_intro,
        "methods": model_methods,
        "results": model_results,
        "discussion": model_discussion,
        "abstract": model_abstract,
        "conclusion": model_conclusion,
    }

    # æ–‡çŒ®åº“é€‰æ‹©ï¼ˆå¤šé€‰ï¼‰- æ”¾åœ¨æœ€å
    st.divider()
    st.subheader("ğŸ“š é€‰æ‹©æ–‡çŒ®åº“")
    output_dir_check = "output"
    if os.path.exists(output_dir_check):
        db_files_tab3 = [
            f.replace(".db", "")
            for f in os.listdir(output_dir_check)
            if f.endswith(".db")
        ]
    else:
        db_files_tab3 = []

    if db_files_tab3:
        selected_db_names_tab3 = st.multiselect(
            "é€‰æ‹©è¦ä½¿ç”¨çš„æ–‡çŒ®åº“ï¼ˆå¯å¤šé€‰ï¼‰",
            options=db_files_tab3,
            default=[],
            help="é€‰ä¸­å¤šä¸ªæ–‡çŒ®åº“ï¼Œç³»ç»Ÿå°†åˆå¹¶æ‰€æœ‰åº“ä¸­çš„æ–‡çŒ®è¿›è¡Œå¼•ç”¨",
            key="db_selector_tab3",
        )
        if selected_db_names_tab3:
            # Calculate total papers from selected databases
            total_papers_count = 0
            paper_details = []
            try:
                from literature import LiteratureDatabaseManager

                for db_name in selected_db_names_tab3:
                    db_path = f"output/{db_name}.db"
                    if os.path.exists(db_path):
                        lit_manager = LiteratureDatabaseManager(db_path)
                        stats = lit_manager.get_statistics()
                        count = stats.get("total_papers", 0)
                        total_papers_count += count
                        paper_details.append(f"**{db_name}**: {count} ç¯‡")
            except Exception:
                paper_details = ["(æ— æ³•è®¡ç®—æ–‡çŒ®æ•°é‡)"]

            # Display selection info
            st.success(
                f"å·²é€‰æ‹© {len(selected_db_names_tab3)} ä¸ªæ–‡çŒ®åº“ï¼Œå…± **{total_papers_count}** ç¯‡æ–‡çŒ®"
            )
            with st.expander("æŸ¥çœ‹å„åº“æ–‡çŒ®æ•°é‡"):
                for detail in paper_details:
                    st.write(detail)
        else:
            st.info("è¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ–‡çŒ®åº“")
    else:
        selected_db_names_tab3 = []
        st.info("æš‚æ— æ–‡çŒ®åº“ï¼Œè¯·å…ˆåœ¨Tab 2å¯¼å…¥æ–‡çŒ®")

    # Check for research data
    has_research_data = st.session_state.doc_analysis_result or uploaded_doc

    # ========== é£æ ¼é€‰é¡¹ç®¡ç† UI ==========
    st.subheader("âš™ï¸ å†™ä½œç« èŠ‚é…ç½®")

    # æ¢å¤å›ºå®šé€‰é¡¹
    if "style_analysis_options" not in st.session_state:
        st.session_state.style_analysis_options = {
            "selected_sections": [
                "abstract",
                "introduction",
                "methods",
                "results",
                "discussion",
                "conclusion",
            ],
            "fixed_options": [
                "abstract",
                "introduction",
                "methods",
                "results",
                "discussion",
                "conclusion",
            ],
        }

    # æ‰€æœ‰å¯ç”¨çš„ç« èŠ‚é€‰é¡¹
    ALL_SECTION_OPTIONS = [
        ("abstract", "ğŸ“„ æ‘˜è¦ (Abstract)"),
        ("introduction", "ğŸ“ å¼•è¨€ (Introduction)"),
        ("methods", "ğŸ”¬ æ–¹æ³• (Methods)"),
        ("results", "ğŸ“Š ç»“æœ (Results)"),
        ("discussion", "ğŸ’¬ è®¨è®º (Discussion)"),
        ("conclusion", "âœ¨ ç»“è®º (Conclusion)"),
    ]

    # æ¢å¤ä¸Šæ¬¡ä¿å­˜çš„é€‰æ‹©
    current_selected = st.session_state.style_analysis_options.get(
        "selected_sections", []
    )

    # ä½¿ç”¨å¤šé€‰æ¡†è®©ç”¨æˆ·é€‰æ‹©è¦å†™ä½œçš„ç« èŠ‚
    col_opts1, col_opts2, col_opts3 = st.columns([2, 1, 1])

    with col_opts1:
        selected_sections = st.multiselect(
            "é€‰æ‹©è¦ç”Ÿæˆçš„ç« èŠ‚",
            options=[(v, l) for v, l in ALL_SECTION_OPTIONS],
            format_func=lambda x: x[1],
            default=[(v, l) for v, l in ALL_SECTION_OPTIONS if v in current_selected],
            key="section_multiselect",
            help="é€‰æ‹©éœ€è¦AIç”Ÿæˆçš„è®ºæ–‡ç« èŠ‚",
        )

    with col_opts2:
        if st.button("âœ… å…¨é€‰", use_container_width=True):
            selected_sections = [(v, l) for v, l in ALL_SECTION_OPTIONS]
            st.session_state.style_analysis_options["selected_sections"] = [
                v for v, l in selected_sections
            ]
            st.rerun()

    with col_opts3:
        if st.button("ğŸ—‘ï¸ æ¸…ç©º", use_container_width=True):
            selected_sections = []
            st.session_state.style_analysis_options["selected_sections"] = []
            st.rerun()

    # ä¿å­˜é€‰æ‹©åˆ°session state
    st.session_state.style_analysis_options["selected_sections"] = [
        v for v, l in selected_sections
    ]

    # æ˜¾ç¤ºå·²é€‰æ‹©çš„ç« èŠ‚æ ‡ç­¾
    if selected_sections:
        st.write("**å·²é€‰æ‹©ç« èŠ‚ï¼š**")
        cols = st.columns(6)
        for i, (v, l) in enumerate(selected_sections):
            with cols[i % 6]:
                # æ·»åŠ å›ºå®š/å–æ¶ˆå›ºå®šæŒ‰é’®
                is_fixed = v in st.session_state.style_analysis_options.get(
                    "fixed_options", []
                )
                fixed_label = "ğŸ“Œ" if is_fixed else "ğŸ”“"
                if st.button(
                    f"{fixed_label} {l.split()[1]}",
                    key=f"fix_{v}",
                    help="ç‚¹å‡»åˆ‡æ¢å›ºå®šçŠ¶æ€",
                ):
                    fixed_list = st.session_state.style_analysis_options.get(
                        "fixed_options", []
                    )
                    if is_fixed:
                        fixed_list.remove(v)
                    else:
                        fixed_list.append(v)
                    st.session_state.style_analysis_options["fixed_options"] = (
                        fixed_list
                    )
                    st.rerun()
    else:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç« èŠ‚")

    # æ˜¾ç¤ºå›ºå®šé€‰é¡¹æç¤º
    fixed_options = st.session_state.style_analysis_options.get("fixed_options", [])
    if fixed_options:
        fixed_labels = [dict(ALL_SECTION_OPTIONS).get(v, v) for v in fixed_options]
        st.caption(f"ğŸ“Œ å›ºå®šé€‰é¡¹: {', '.join(fixed_labels)}")

    if st.button("ğŸš€ å¼€å§‹å…¨è‡ªåŠ¨å†™ä½œ", type="primary", use_container_width=True):
        if not has_research_data:
            st.warning("è¯·å…ˆä¸Šä¼ ç ”ç©¶æ–‡æ¡£")
        elif not selected_sections:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç« èŠ‚")
        else:
            # Create progress display
            progress_container = st.container()
            with progress_container:
                st.subheader("Writing Progress")

                progress_bar = st.progress(0)
                status_area = st.empty()
                log_area = st.empty()
                log_messages = []

                def progress_callback(step, total, section, progress):
                    """Progress callback for coordinator"""
                    status_area.text(f"æ­¥éª¤ {step}/{total}: æ­£åœ¨æ’°å†™ {section}...")
                    progress_bar.progress(progress)

                    section_names = {
                        "introduction": "å¼•è¨€",
                        "methods": "æ–¹æ³•",
                        "results": "ç»“æœ",
                        "discussion": "è®¨è®º",
                        "abstract": "æ‘˜è¦ï¼ˆä¸€çº§AIå…¨å±€è§†è§’ï¼‰",
                        "conclusion": "ç»“è®ºï¼ˆä¸€çº§AIå…¨å±€è§†è§’ï¼‰",
                    }
                    log_messages.append(
                        f"âœ“ æ­£åœ¨æ’°å†™ {section_names.get(section, section)}..."
                    )
                    log_area.info("\n".join(log_messages[-5:]))

                # Step 1: Initialize
                status_area.text("æ­£åœ¨åˆå§‹åŒ–é…ç½®...")
                progress_bar.progress(5)
                log_area.info("å¼€å§‹è®ºæ–‡å†™ä½œæµç¨‹...")

                try:
                    # Import coordinator
                    from coordinator import MultiAgentCoordinator
                    from integrator import DraftIntegrator

                    # Load detailed chapter-specific style guides
                    style_guide_dir = "output/style"
                    chapter_guides = {}

                    # Try to load individual chapter guides
                    guide_files = {
                        "abstract": "abstract_guide.md",
                        "introduction": "introduction_guide.md",
                        "methods": "methods_guide.md",
                        "results": "results_guide.md",
                        "discussion": "discussion_guide.md",
                        "conclusion": "conclusion_guide.md",
                    }

                    for chapter, filename in guide_files.items():
                        guide_path = os.path.join(style_guide_dir, filename)
                        if os.path.exists(guide_path):
                            with open(guide_path, "r", encoding="utf-8") as f:
                                chapter_guides[chapter] = f.read()
                        else:
                            chapter_guides[chapter] = (
                                f"# {chapter.title()} Writing Guide\n\nNo specific guide available for this chapter."
                            )

                    # Create comprehensive style guide by combining all chapter guides
                    comprehensive_guide = f"""# Comprehensive Journal Style Guide

## Overview
This guide provides detailed writing instructions for each section of papers published in the target journal.

## Section-Specific Guidelines

"""

                    for chapter, content in chapter_guides.items():
                        comprehensive_guide += (
                            f"\n## {chapter.title()} Section\n{content}\n"
                        )

                    # Add general style information from summary if available
                    summary_path = os.path.join(style_guide_dir, "style_summary.md")
                    if os.path.exists(summary_path):
                        with open(summary_path, "r", encoding="utf-8") as f:
                            comprehensive_guide += (
                                f"\n## General Style Summary\n{f.read()}\n"
                            )

                    # Use comprehensive guide as style_guide content
                    style_guide_content = comprehensive_guide

                    # Load background content from document analysis
                    background_content = ""
                    if st.session_state.doc_analysis_result:
                        # Use extracted text from Word document
                        result = st.session_state.doc_analysis_result
                        text_parts = []
                        for elem in result.text_elements:
                            if elem.element_type == "Title":
                                text_parts.append(f"# {elem.text}")
                            elif elem.element_type == "Table":
                                text_parts.append(f"\n[è¡¨æ ¼æ•°æ®]\n{elem.text}")
                            else:
                                text_parts.append(elem.text)
                        background_content = "\n\n".join(text_parts)
                        log_messages.append("âœ“ å·²ä½¿ç”¨Wordæ–‡æ¡£è§£æçš„æ–‡æœ¬å†…å®¹")

                    # Fallback: use default content
                    if not background_content:
                        background_content = "è¯·æ ¹æ®æä¾›çš„ç ”ç©¶èƒŒæ™¯æ’°å†™å†…å®¹ã€‚"
                        log_messages.append("âš  æœªæ£€æµ‹åˆ°æ–‡æ¡£å†…å®¹ï¼Œå°†ä½¿ç”¨é»˜è®¤èƒŒæ™¯")

                    # Load results data from corrected content or file
                    results_data = ""

                    # Priority 1: Use corrected content from document analysis
                    if st.session_state.get("corrected_content"):
                        corrected = st.session_state.corrected_content
                        results_parts = []

                        # Add table descriptions
                        if corrected.table_descriptions:
                            results_parts.append("ã€è¡¨æ ¼åˆ†æç»“æœã€‘")
                            for table_id, desc in corrected.table_descriptions.items():
                                results_parts.append(f"\n{table_id}:\n{desc}")

                        # Add image descriptions (from session state)
                        if st.session_state.doc_analysis_result:
                            for (
                                img
                            ) in st.session_state.doc_analysis_result.image_elements:
                                results_parts.append(
                                    f"\n{img.image_id} (å›¾è¡¨):\n{img.description}"
                                )

                        if results_parts:
                            results_data = "\n".join(results_parts)
                            log_messages.append("âœ“ å·²ä½¿ç”¨ä¿®æ­£åçš„å›¾è¡¨/è¡¨æ ¼åˆ†æç»“æœ")

                    # Step 2: Create coordinator with user-provided API config
                    progress_bar.progress(10)
                    status_area.text("æ­£åœ¨åˆå§‹åŒ–AIåè°ƒå™¨...")

                    # Validate API configuration
                    if not api_url or not api_key:
                        st.error("è¯·åœ¨ä¾§è¾¹æ è¾“å…¥APIåœ°å€å’ŒAPI Key")
                        raise Exception("ç¼ºå°‘APIé…ç½®")

                    coordinator = MultiAgentCoordinator(
                        base_url=api_url, api_key=api_key, model_config=model_config
                    )

                    # Load literature from multiple databases
                    literature_papers = []
                    if selected_db_names_tab3:
                        try:
                            from literature import LiteratureDatabaseManager

                            total_papers = 0
                            for db_name in selected_db_names_tab3:
                                db_path = f"output/{db_name}.db"
                                if os.path.exists(db_path):
                                    lit_manager = LiteratureDatabaseManager(db_path)
                                    papers = lit_manager.search("", limit=100)
                                    literature_papers.extend(papers)
                                    total_papers += len(papers)
                                    log_messages.append(
                                        f"âœ“ ä» {db_name} åŠ è½½äº† {len(papers)} ç¯‡æ–‡çŒ®"
                                    )
                            log_messages.append(
                                f"âœ“ å…±åŠ è½½ {total_papers} ç¯‡æ–‡çŒ®ç”¨äºå¼•ç”¨"
                            )
                        except Exception as e:
                            st.warning(f"åŠ è½½æ–‡çŒ®æ•°æ®åº“å¤±è´¥: {e}")
                    else:
                        st.info("æœªé€‰æ‹©æ–‡çŒ®åº“ï¼Œå°†ä¸ä½¿ç”¨æ–‡çŒ®å¼•ç”¨åŠŸèƒ½")

                    # Get citation style from style analysis if available
                    citation_style = {
                        "citation_type": "author-year",
                        "reference_format": "nature",
                    }
                    style_summary_path = os.path.join(
                        style_guide_dir, "style_summary.md"
                    )
                    if os.path.exists(style_summary_path):
                        try:
                            import json

                            report_path = os.path.join(
                                style_guide_dir, "journal_style_report.json"
                            )
                            if os.path.exists(report_path):
                                with open(report_path, "r", encoding="utf-8") as f:
                                    report_data = json.load(f)
                                    if "citation_style" in report_data:
                                        citation_style = report_data["citation_style"]
                                        log_messages.append(
                                            f"âœ“ å·²åŠ è½½å¼•ç”¨é£æ ¼: {citation_style.get('citation_type', 'author-year')}"
                                        )
                        except Exception as e:
                            log_messages.append(f"âš  æ— æ³•åŠ è½½å¼•ç”¨é£æ ¼é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")

                    # Prepare context
                    context = {
                        "background": background_content,
                        "style_guide": style_guide_content,
                        "citation_style": citation_style,
                        "literature": literature_papers,
                        "results_data": results_data,
                        "target_journal": journal,
                    }

                    # Get selected sections from session state
                    selected_sections = st.session_state.style_analysis_options.get(
                        "selected_sections", None
                    )

                    # Step 3-8: Run coordinator workflow
                    status_area.text("æ­£åœ¨è¿è¡ŒAIå†™ä½œæµç¨‹...")
                    progress_bar.progress(15)

                    results = coordinator.run_workflow(
                        context=context,
                        progress_callback=progress_callback,
                        sections=selected_sections,
                    )

                    # Save sections
                    os.makedirs(os.path.join(output_dir, "sections"), exist_ok=True)
                    for section_name, result in results.items():
                        section_path = os.path.join(
                            output_dir, "sections", f"{section_name}.md"
                        )
                        with open(section_path, "w", encoding="utf-8") as f:
                            f.write(result.content)
                        log_messages.append(
                            f"âœ“ Saved {section_name}: {result.word_count} words"
                        )

                    progress_bar.progress(95)
                    status_area.text("Integrating all sections...")

                    # Step 9: Integrate sections
                    sections_dict = {
                        "introduction": os.path.join(
                            output_dir, "sections", "introduction.md"
                        ),
                        "methods": os.path.join(output_dir, "sections", "methods.md"),
                        "results": os.path.join(output_dir, "sections", "results.md"),
                        "discussion": os.path.join(
                            output_dir, "sections", "discussion.md"
                        ),
                    }

                    integrator = DraftIntegrator()
                    sections_content = integrator.collect_sections(sections_dict)
                    draft, report = integrator.integrate(sections_content)

                    # Save final draft
                    os.makedirs(os.path.join(output_dir, "final"), exist_ok=True)
                    final_draft_path = os.path.join(
                        output_dir, "final", "final_draft.md"
                    )
                    with open(final_draft_path, "w", encoding="utf-8") as f:
                        f.write(draft)

                    # Save report
                    report_path = os.path.join(output_dir, "final", "draft_report.json")
                    integrator.save_report(report, report_path)

                    # Step 10: Quality check
                    progress_bar.progress(100)
                    status_area.text("è´¨é‡æ£€æŸ¥å®Œæˆï¼")

                    # Show success message
                    st.success(
                        f"""
                        **è®ºæ–‡å†™ä½œå®Œæˆï¼**

                        **è¾“å…¥å†…å®¹ï¼š**
                        - ç ”ç©¶æ–‡æ¡£: {uploaded_doc.name if uploaded_doc else "ä½¿ç”¨å·²è§£ææ–‡æ¡£"}

                        **ç”Ÿæˆçš„æ–‡ä»¶ï¼š**
                        - `output/sections/abstract.md` - æ‘˜è¦ï¼ˆä¸€çº§AIå…¨å±€è§†è§’ï¼‰
                        - `output/sections/introduction.md` - å¼•è¨€
                        - `output/sections/methods.md` - æ–¹æ³•
                        - `output/sections/results.md` - ç»“æœ
                        - `output/sections/discussion.md` - è®¨è®º
                        - `output/sections/conclusion.md` - ç»“è®ºï¼ˆä¸€çº§AIå…¨å±€è§†è§’ï¼‰
                        - `output/final/final_draft.md` - æ•´åˆåçš„å®Œæ•´è‰ç¨¿

                        **è´¨é‡æŠ¥å‘Šï¼š**
                        - æ€»å­—æ•°: {report.total_words}
                        - è´¨é‡è¯„åˆ†: {report.overall_quality_score:.2f}
                        - å‘ç°é—®é¢˜: {report.consistency_report.get("total_issues", 0)}
                        """
                    )

                    # Show file list
                    with st.expander("æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶"):
                        sections_dir = os.path.join(output_dir, "sections")
                        final_dir = os.path.join(output_dir, "final")

                        if os.path.exists(sections_dir):
                            st.write("**å„ç« èŠ‚ï¼š**")
                            for f in sorted(os.listdir(sections_dir)):
                                st.write(f"  ğŸ“„ {f}")

                        if os.path.exists(final_dir):
                            st.write("**æœ€ç»ˆè‰ç¨¿ï¼š**")
                            for f in os.listdir(final_dir):
                                st.write(f"  ğŸ“„ {f}")

                except Exception as e:
                    st.error(f"å†™ä½œå¤±è´¥: {str(e)}")
                    import traceback

                    with st.expander("é”™è¯¯è¯¦æƒ…"):
                        st.code(traceback.format_exc())

        # Note: ä¸´æ—¶æ–‡ä»¶ä¼šåœ¨ä¼šè¯ç»“æŸæ—¶è‡ªåŠ¨æ¸…ç†

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        st.info("ğŸ’¡ ä¸´æ—¶æ–‡ä»¶ä¼šåœ¨ä¼šè¯ç»“æŸæ—¶è‡ªåŠ¨æ¸…ç†")

# ========== Tab 4: äº†è§£æ›´å¤š ==========
with tab4:
    st.header("ğŸ’¡ é¡¹ç›®æ¶æ„ä¸AIæ¨¡å‹")
    st.markdown("äº†è§£æ•´ä¸ªè®ºæ–‡å†™ä½œç³»ç»Ÿçš„å·¥ä½œæµç¨‹å’Œæ¯ä¸ªé˜¶æ®µä½¿ç”¨çš„AIæ¨¡å‹")

    # å·¥ä½œæµç¨‹å›¾
    st.subheader("ğŸ“Š å®Œæ•´å·¥ä½œæµç¨‹")

    workflow = """
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        è®ºæ–‡å†™ä½œåŠ©æ‰‹ - ç³»ç»Ÿæ¶æ„                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    è¾“å…¥å±‚                          å¤„ç†å±‚                           è¾“å‡ºå±‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚                        â”‚          â”‚          â”‚
    â”‚  ç ”ç©¶èƒŒæ™¯ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  1. èƒŒæ™¯è§£æå™¨          â”‚          â”‚          â”‚
    â”‚  (PDF/   â”‚          â”‚  AI: Claude-Sonnet-4   â”‚          â”‚  æ‘˜è¦    â”‚
    â”‚   Word)  â”‚          â”‚  æå–ç ”ç©¶ä¸»é¢˜/æ–¹æ³•/æ•°æ® â”‚          â”‚  (ä¸€çº§AI) â”‚
    â”‚          â”‚          â”‚                        â”‚          â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  å¼•è¨€    â”‚
           â”‚                        â”‚                        â”‚DeepSeek  â”‚
           â”‚                        â–¼                        â”‚          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”‚
    â”‚          â”‚          â”‚  2. æ–‡çŒ®æ•°æ®åº“          â”‚          â”‚  æ–¹æ³•    â”‚
    â”‚  æ–‡çŒ®Excelâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  AI: Claude-Sonnet-4   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Claude-   â”‚
    â”‚  (WOS)   â”‚          â”‚  ç²¾å‡†è¯†åˆ«æ¯ä¸ªå•å…ƒæ ¼     â”‚          â”‚  Sonnet  â”‚
    â”‚          â”‚          â”‚                        â”‚          â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â”‚
           â”‚                        â”‚                        â”‚  ç»“æœ    â”‚
           â”‚                        â–¼                        â”‚GPT-4o    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”‚
    â”‚          â”‚          â”‚  3. é£æ ¼åˆ†æå™¨          â”‚          â”‚          â”‚
    â”‚  èŒƒæ–‡PDF â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  AI: GPT-4o            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  è®¨è®º    â”‚
    â”‚          â”‚          â”‚  æå–è¯æ±‡/æ—¶æ€/è¿‡æ¸¡è¯   â”‚          â”‚Claude-   â”‚
    â”‚          â”‚          â”‚                        â”‚          â”‚  3.5     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â”‚
           â”‚                        â”‚                        â”‚          â”‚
           â”‚                        â–¼                        â”‚  ç»“è®º    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  (ä¸€çº§AI) â”‚
    â”‚          â”‚          â”‚  4. åè°ƒå™¨(Coordinator) â”‚          â”‚          â”‚
    â”‚          â”‚          â”‚  ç®¡ç†6ä¸ªå†™ä½œAgent       â”‚          â”‚ å®Œæ•´è®ºæ–‡ â”‚
    â”‚          â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â˜…ä¸€çº§AIå…¨å±€è§†è§’â˜…     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  (æœ€ç»ˆ   â”‚
    â”‚          â”‚          â”‚                        â”‚          â”‚   è‰ç¨¿)  â”‚
    â”‚          â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â”‚
    â”‚          â”‚                        â”‚                    â”‚          â”‚
    â”‚          â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”‚
    â”‚          â”‚          â”‚  5. è‰ç¨¿æ•´åˆå™¨          â”‚          â”‚          â”‚
    â”‚          â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  AI: Claude-Sonnet-4   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚          â”‚
    â”‚          â”‚          â”‚  æ£€æŸ¥ä¸€è‡´æ€§/å¢å¼ºè¿‡æ¸¡   â”‚          â”‚          â”‚
    â”‚          â”‚          â”‚                        â”‚          â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    æ³¨: æ‘˜è¦(Abstract)å’Œç»“è®º(Conclusion)ç”±ä¸€çº§AI(Coordinator)æ ¹æ®å…¨æ–‡å…¨å±€è§†è§’æ’°å†™
    ```
    """
    st.markdown(workflow)

    # AIæ¨¡å‹ä½¿ç”¨è¯¦æƒ…
    st.subheader("ğŸ¤– å„é˜¶æ®µAIæ¨¡å‹è¯¦æƒ…")

    ai_details = """
    | é˜¶æ®µ | ä»»åŠ¡ | AIæ¨¡å‹ | æä¾›å•† | æˆæœ¬/1K tokens | é€‰æ‹©ç†ç”± |
    |------|------|--------|--------|---------------|----------|
    | é£æ ¼åˆ†æ | æå–è¯æ±‡/æ—¶æ€/è¿‡æ¸¡è¯ | **GPT-4o** | OpenAI | $0.02 | å¤æ‚æ¨¡å¼è¯†åˆ«æœ€å¼º |
    | æ–‡çŒ®å¯¼å…¥ | ç²¾å‡†è¯†åˆ«Excelå•å…ƒæ ¼ | **Claude-Sonnet-4** | Anthropic | $0.018 | ç»“æ„åŒ–æå–æœ€å‡† |
    | å¼•è¨€å†™ä½œ | åˆ›æ„èƒŒæ™¯å™è¿° | **DeepSeek-Chat** | DeepSeek | $0.00042 | æ€§ä»·æ¯”æœ€é«˜ |
    | æ–¹æ³•å†™ä½œ | æŠ€æœ¯æ–‡æ¡£æ’°å†™ | **Claude-Sonnet-4** | Anthropic | $0.018 | æŠ€æœ¯æœ¯è¯­æœ€ç²¾ç¡® |
    | ç»“æœå†™ä½œ | æ•°æ®æè¿°/ç»Ÿè®¡ | **GPT-4o** | OpenAI | $0.02 | æ•°å€¼æ¨ç†æœ€å¼º |
    | è®¨è®ºå†™ä½œ | è®ºè¯/ç»¼åˆåˆ†æ | **Claude-3.5-Sonnet** | Anthropic | $0.018 | è®ºè¯èƒ½åŠ›æœ€å¼º |
    | â­ æ‘˜è¦ | å…¨å±€æ€»ç»“ï¼ˆæ ¹æ®å…¨æ–‡ï¼‰ | **Claude-Sonnet-4** | Anthropic | $0.018 | ä¸€çº§AIå…¨å±€è§†è§’ |
    | â­ ç»“è®º | ç»¼åˆåˆ†æï¼ˆæ ¹æ®å…¨æ–‡ï¼‰ | **Claude-Sonnet-4** | Anthropic | $0.018 | ä¸€çº§AIå…¨å±€è§†è§’ |
    | è‰ç¨¿æ•´åˆ | ä¸€è‡´æ€§æ£€æŸ¥/è¿‡æ¸¡ | **Claude-Sonnet-4** | Anthropic | $0.018 | è´¨é‡æ£€æŸ¥æœ€ä»”ç»† |
    
    â­ è¡¨ç¤ºç”±ä¸€çº§AIï¼ˆCoordinatorï¼‰åŸºäºå…¨å±€è§†è§’æ’°å†™
    """
    st.markdown(ai_details)

    # æˆæœ¬å¯¹æ¯”
    st.subheader("ğŸ’° æˆæœ¬ä¼˜åŒ–ç­–ç•¥")

    cost_info = """
    ### ä¸ºä»€ä¹ˆè¿™æ ·é€‰æ‹©ï¼Ÿ
    
    - **DeepSeek** (å†™ä½œä»»åŠ¡): ä»·æ ¼ä»…ä¸ºGPT-4oçš„1/50ï¼Œä½†å†™ä½œè´¨é‡ç›¸å½“
    - **Claude** (åˆ†æä»»åŠ¡): ç»“æ„åŒ–ç†è§£èƒ½åŠ›æœ€å¼ºï¼Œé€‚åˆæå–å’Œåˆ†æ
    - **GPT-4o** (æ•°å€¼ä»»åŠ¡): æ¨ç†èƒ½åŠ›æœ€å¼ºï¼Œé€‚åˆæ•°æ®æè¿°
    
    ### æˆæœ¬å¯¹æ¯”
    
    | æ–¹æ¡ˆ | æ¯ç¯‡è®ºæ–‡æˆæœ¬ | 10ç¯‡æˆæœ¬ |
    |------|------------|---------|
    | å…¨éƒ¨ä½¿ç”¨GPT-4o | ~$0.50 | $5.00 |
    | å…¨éƒ¨ä½¿ç”¨Claude | ~$0.40 | $4.00 |
    | **æˆ‘ä»¬çš„æ··åˆæ–¹æ¡ˆ** | **~$0.07** | **~$0.70** |
    
    **èŠ‚çœçº¦85%æˆæœ¬ï¼**
    """
    st.markdown(cost_info)

    # æŠ€æœ¯æ ˆ
    st.subheader("ğŸ› ï¸ æŠ€æœ¯æ ˆ")

    tech_stack = """
    ### å‰ç«¯
    - **Streamlit** - ç½‘é¡µç•Œé¢
    
    ### åç«¯
    - **Python** - ä¸»è¦å¼€å‘è¯­è¨€
    - **SpaCy** - NLPæ–‡æœ¬å¤„ç†
    - **NLTK** - è‡ªç„¶è¯­è¨€å¤„ç†
    
    ### æ–‡æ¡£å¤„ç†
    - **pdfplumber** - PDFè§£æ
    - **python-docx** - Wordè§£æ
    - **openpyxl** - Excelè§£æ
    
    ### APIä»£ç†
    - æœ¬åœ°ä»£ç†: `http://127.0.0.1:13148/v1`
    - æ”¯æŒ: OpenAI, Anthropic, DeepSeekç­‰å¤šç§æ¨¡å‹
    """
    st.markdown(tech_stack)

    # ä½¿ç”¨æµç¨‹
    st.subheader("ğŸ“– ä½¿ç”¨æµç¨‹")

    usage_flow = """
    ### ä¸‰æ­¥å®Œæˆè®ºæ–‡å†™ä½œ
    
    1. **ä¸Šä¼ æ–‡ä»¶**
        - ç ”ç©¶èƒŒæ™¯ (PDF/Word/Markdown)
        - ç ”ç©¶æˆæœ (Excel/å›¾ç‰‡/æè¿°) - å¯é€‰
        - æ–‡çŒ®æ•°æ®åº“ (Excel) - å¯é€‰
        - èŒƒæ–‡æ ·æœ¬ (PDF) - å¯é€‰
    
    2. **ç‚¹å‡»å¼€å§‹**
       - ç³»ç»Ÿè‡ªåŠ¨å®Œæˆæ‰€æœ‰æ­¥éª¤
       - æ— éœ€æ‰‹åŠ¨å¹²é¢„
    
    3. **ä¸‹è½½è®ºæ–‡**
       - å„ç« èŠ‚ç‹¬ç«‹æ–‡ä»¶ï¼ˆæ‘˜è¦/å¼•è¨€/æ–¹æ³•/ç»“æœ/è®¨è®º/ç»“è®ºï¼‰
       - æ•´åˆåçš„å®Œæ•´è‰ç¨¿
    
    ### ç”Ÿæˆçš„å…¨éƒ¨ç« èŠ‚
    1. **æ‘˜è¦ (Abstract)** - ä¸€çº§AIå…¨å±€è§†è§’æ€»ç»“
    2. **å¼•è¨€ (Introduction)** - DeepSeekæ’°å†™
    3. **æ–¹æ³• (Methods)** - Claude-Sonnet-4æ’°å†™
    4. **ç»“æœ (Results)** - GPT-4oæ’°å†™
    5. **è®¨è®º (Discussion)** - Claude-3.5-Sonnetæ’°å†™
    6. **ç»“è®º (Conclusion)** - ä¸€çº§AIå…¨å±€è§†è§’æ€»ç»“
    
    ### æ³¨æ„äº‹é¡¹
    - ç¡®ä¿APIä»£ç†æ­£åœ¨è¿è¡Œ
    - API Keyå·²é¢„å¡«åœ¨ä¾§è¾¹æ 
    - å¯é€‰æ­¥éª¤ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
    """
    st.markdown(usage_flow)

    # æ‘˜è¦å’Œç»“è®ºä¸ºä»€ä¹ˆé‡è¦
    st.subheader("ğŸ“ ä¸ºä»€ä¹ˆæ‘˜è¦å’Œç»“è®ºç”±ä¸€çº§AIæ’°å†™ï¼Ÿ")

    abstract_reason = """
    ### ä¸€çº§AIï¼ˆCoordinatorï¼‰çš„å…¨å±€è§†è§’ä¼˜åŠ¿
    
    **ä¼ ç»Ÿåšæ³•ï¼š**
    - å„ç« èŠ‚ç”±ä¸åŒAIç‹¬ç«‹æ’°å†™
    - æ‘˜è¦å’Œç»“è®ºå®¹æ˜“ä¸æ­£æ–‡è„±èŠ‚
    - éš¾ä»¥å½¢æˆç»Ÿä¸€çš„è®ºè¿°ä¸»çº¿
    
    **æˆ‘ä»¬çš„æ–¹æ¡ˆï¼š**
    - ä¸€çº§AIåœ¨æ‰€æœ‰ç« èŠ‚å®Œæˆåæ’°å†™æ‘˜è¦å’Œç»“è®º
    - å…·æœ‰å®Œæ•´çš„å…¨æ–‡ä¸Šä¸‹æ–‡
    - èƒ½å‡†ç¡®æ€»ç»“ç ”ç©¶ç›®çš„ã€æ–¹æ³•ã€ä¸»è¦å‘ç°å’Œè´¡çŒ®
    - ç¡®ä¿æ‘˜è¦ä¸æ­£æ–‡é«˜åº¦ä¸€è‡´
    
    **Claude-Sonnet-4çš„ä¼˜åŠ¿ï¼š**
    - 200K tokenä¸Šä¸‹æ–‡çª—å£
    - å¼ºå¤§çš„é•¿æ–‡æœ¬ç†è§£å’Œç»¼åˆèƒ½åŠ›
    - èƒ½å¤Ÿä»å…¨å±€è§’åº¦æç‚¼å…³é”®ä¿¡æ¯
    - ç”Ÿæˆç»“æ„æ¸…æ™°ã€é‡ç‚¹çªå‡ºçš„æ‘˜è¦å’Œç»“è®º
    """
    st.markdown(abstract_reason)

# åº•éƒ¨ä¿¡æ¯
st.divider()
st.markdown(
    """
<div style='text-align: center; color: gray;'>
    <p>ğŸ“ è®ºæ–‡å†™ä½œåŠ©æ‰‹ | åŸºäºå¤šä»£ç†AIç³»ç»Ÿ</p>
    <p>åªéœ€ä¸Šä¼ èƒŒæ™¯æ–‡ä»¶ï¼Œç‚¹å‡»"å¼€å§‹å†™ä½œ"å³å¯è‡ªåŠ¨å®Œæˆå…¨æ–‡</p>
</div>
""",
    unsafe_allow_html=True,
)
